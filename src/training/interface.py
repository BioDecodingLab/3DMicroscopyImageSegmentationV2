from pathlib import Path
from typing import Annotated

from loguru import logger
import tensorflow
import typer

from src.config import (
    BATCH_SIZE,
    INTENSITY_PARAMS,
    LEARNING_RATE,
    LOSS_FUNCTION,
    METRICS,
    NUM_EPOCHS,
    RANDOM_SEED,
)
from src.dataset_paths import DatasetPaths
from src.training.dataset import ImageDataset
import src.training.models as models_module
from src.training.utils import (
    configure_gpu,
    create_callbacks,
    get_model_from_class,
    get_patch_paths_for_training,
    set_random_seed,
)

configure_gpu()


def interface(
    dataset_path: Annotated[Path, typer.Argument(help="Path to dataset root")],
    model_name: Annotated[str, typer.Argument(help="Model: UNet3D or AttentionUNet3D")],
    augmentation: Annotated[str, typer.Argument(help="Augmentation: NONE, STANDARD, or OURS")],
    psf_path: Annotated[Path, typer.Option(help="Path to PSF file for OURS augmentation")] = None,
    reproducibility: Annotated[
        bool, typer.Option(help="Enable reproducibility by setting random seed")
    ] = True,
):
    """Train a 3D segmentation model on microscopy patches.

    Uses patches generated by the patches module. Saves model checkpoints
    and TensorBoard logs to the dataset's models/ and logs/ directories.
    """
    # Initialize paths from dataset root
    paths = DatasetPaths(dataset_path)
    paths.validate_for_training()

    # Enable reproducibility if requested
    if reproducibility:
        logger.info(f"Setting random seed to {RANDOM_SEED}")
        set_random_seed(RANDOM_SEED)
    else:
        logger.info("Reproducibility disabled")

    # Set PSF file if provided
    if psf_path and psf_path.exists():
        INTENSITY_PARAMS.update({"use_psf": True, "psf_path": str(psf_path)})
        logger.info(f"PSF file activated: {psf_path}")
    else:
        INTENSITY_PARAMS.update({"use_psf": False})
        logger.info("No PSF file provided")

    # Get patch paths from nested structure
    train_image_paths, val_image_paths, train_mask_paths, val_mask_paths = (
        get_patch_paths_for_training(paths.training_regular_patches)
    )

    train_dataset = ImageDataset(
        image_paths=train_image_paths,
        mask_paths=train_mask_paths,
        batch_size=BATCH_SIZE,
        augmentation=augmentation,
        intensity_params=INTENSITY_PARAMS,
    )

    val_dataset = ImageDataset(
        image_paths=val_image_paths,
        mask_paths=val_mask_paths,
        batch_size=BATCH_SIZE,
        augmentation="NONE",  # No augmentation for validation
    )

    logger.info(f"Creating {model_name} model...")
    model_class = get_model_from_class(model_name, models_module)
    model = model_class().build_model()

    # Create optimizer
    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=LEARNING_RATE)

    # Compile model
    model.compile(optimizer=optimizer, loss=LOSS_FUNCTION, metrics=METRICS)

    # Create callbacks with derived paths
    callbacks = create_callbacks(
        model_name=model_name,
        augmentation=augmentation,
        logs_dir=paths.logs,
        models_dir=paths.models,
    )

    logger.info("Starting training")
    model.fit(
        train_dataset,
        validation_data=val_dataset,
        epochs=NUM_EPOCHS,
        callbacks=callbacks,
        verbose=1,
    )

    logger.success("Training complete!")


if __name__ == "__main__":
    typer.run(interface)
